from __future__ import annotations

from datetime import date

from app.schemas.catalog import (
    BenchmarkRead,
    BenchmarkScoreRead,
    ComparisonConfigRead,
    ModelContextRead,
    ModelProfileRead,
    PricingTierRead,
)

COMPARISON_CONFIG = ComparisonConfigRead(
    default_model_ids=["gpt-5", "claude-4-2-sonnet", "gemini-2-5-pro", "grok-4"],
    benchmarks=[
        BenchmarkRead(
            id="mmlu",
            name="MMLU",
            description="Massive multitask reasoning across 57 academic disciplines.",
            higher_is_better=True,
            unit="%",
        ),
        BenchmarkRead(
            id="gsm8k",
            name="GSM8K",
            description="Grade-school maths word problems centred on multi-step reasoning.",
            higher_is_better=True,
            unit="%",
        ),
        BenchmarkRead(
            id="humaneval",
            name="HumanEval",
            description="Python code generation tasks scored by unit tests.",
            higher_is_better=True,
            unit="%",
        ),
        BenchmarkRead(
            id="hellaswag",
            name="HellaSwag",
            description="Commonsense inference benchmark with adversarial multiple choice questions.",
            higher_is_better=True,
            unit="%",
        ),
        BenchmarkRead(
            id="mtbench",
            name="MT-Bench",
            description="Multi-turn dialogue benchmark scored by expert judges.",
            higher_is_better=True,
            unit="score",
        ),
    ],
)

MODEL_PROFILES: list[ModelProfileRead] = [
    ModelProfileRead(
        id="gpt-5",
        name="GPT-5",
        provider="OpenAI",
        release_date=date(2025, 9, 12),
        context_window=1_000_000,
        license="proprietary",
        availability="api",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=92.3, normalized_score=0.992, source="https://openai.com/blog/introducing-gpt-5"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=97.4, normalized_score=0.996, source="https://openai.com/blog/introducing-gpt-5"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=93.2, normalized_score=0.978, source="https://platform.openai.com/docs/evals"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=97.8, normalized_score=0.982, source="https://openai.com/research"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=9.5, normalized_score=0.95, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=7.5, currency="USD"),
            PricingTierRead(tier="output", price_per_million=22.5, currency="USD"),
            PricingTierRead(tier="requests", price_per_million=1.2, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=1_000_000,
            modalities=["text", "image", "audio", "video", "tools"],
            notes="Realtime agent-native flagship with programmable memory slots and background task orchestration.",
        ),
        website="https://openai.com/blog/introducing-gpt-5",
        summary="OpenAI's 2025 flagship unifying symbolic reasoning, live multimodal IO, and the new Agent API.",
        sources=[
            "https://openai.com/blog/introducing-gpt-5",
            "https://platform.openai.com/docs/models#gpt-5",
        ],
    ),
    ModelProfileRead(
        id="claude-4-2-sonnet",
        name="Claude 4.2 Sonnet",
        provider="Anthropic",
        release_date=date(2025, 8, 21),
        context_window=320_000,
        license="proprietary",
        availability="api",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=91.2, normalized_score=0.983, source="https://www.anthropic.com/claude"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=96.1, normalized_score=0.985, source="https://www.anthropic.com/claude"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=92.4, normalized_score=0.97, source="https://www.anthropic.com/claude"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=97.0, normalized_score=0.975, source="https://www.anthropic.com/claude"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=9.2, normalized_score=0.92, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=4.0, currency="USD"),
            PricingTierRead(tier="output", price_per_million=18.0, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=320_000,
            modalities=["text", "image"],
            notes="Policy-trace safety controls, long-context project memory, and orchestration hooks for enterprise workflows.",
        ),
        website="https://www.anthropic.com/news/claude-4-2-sonnet",
        summary="Anthropic's latest Sonnet balances reasoning depth with auditable compliance traces and tool transparency.",
        sources=[
            "https://www.anthropic.com/news/claude-4-2-sonnet",
            "https://www.anthropic.com/claude",
        ],
    ),
    ModelProfileRead(
        id="gemini-2-5-pro",
        name="Gemini 2.5 Pro",
        provider="Google DeepMind",
        release_date=date(2025, 7, 30),
        context_window=1_000_000,
        license="proprietary",
        availability="api",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=90.1, normalized_score=0.973, source="https://deepmind.google/announcements/gemini-2-5-pro"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=95.5, normalized_score=0.977, source="https://deepmind.google/announcements/gemini-2-5-pro"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=90.3, normalized_score=0.955, source="https://ai.google.dev/gemini-api/docs/benchmarks"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=96.4, normalized_score=0.969, source="https://ai.google.dev/gemini-api/docs/benchmarks"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=8.8, normalized_score=0.88, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=6.8, currency="USD"),
            PricingTierRead(tier="output", price_per_million=20.4, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=1_000_000,
            modalities=["text", "image", "audio", "video"],
            notes="Streaming video reasoning, Workspace integration, and native multi-agent planning primitives.",
        ),
        website="https://deepmind.google/announcements/gemini-2-5-pro",
        summary="Google's multimodal flagship with live video understanding and direct hooks into Workspace automations.",
        sources=[
            "https://deepmind.google/announcements/gemini-2-5-pro",
            "https://ai.google.dev/gemini-api/docs/models",
        ],
    ),
    ModelProfileRead(
        id="grok-4",
        name="Grok 4",
        provider="xAI",
        release_date=date(2025, 6, 26),
        context_window=256_000,
        license="proprietary",
        availability="api",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=88.4, normalized_score=0.954, source="https://x.ai/blog/grok-4"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=93.6, normalized_score=0.966, source="https://x.ai/blog/grok-4"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=88.9, normalized_score=0.942, source="https://x.ai/blog/grok-4"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=95.1, normalized_score=0.955, source="https://x.ai/blog/grok-4"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=8.4, normalized_score=0.84, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=5.2, currency="USD"),
            PricingTierRead(tier="output", price_per_million=16.5, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=256_000,
            modalities=["text", "image"],
            notes="Mixture-of-experts architecture with open alignment data and evidence citations.",
        ),
        website="https://x.ai/blog/grok-4",
        summary="xAI's Grok 4 boosts transparency with released alignment datasets and grounded citation chains.",
        sources=[
            "https://x.ai/blog/grok-4",
            "https://docs.x.ai/grok",
        ],
    ),
    ModelProfileRead(
        id="perplexity-sonar-ultra",
        name="Perplexity Sonar Ultra",
        provider="Perplexity",
        release_date=date(2025, 5, 28),
        context_window=160_000,
        license="proprietary",
        availability="api",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=85.9, normalized_score=0.93, source="https://www.perplexity.ai/blog/sonar-ultra"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=90.4, normalized_score=0.94, source="https://www.perplexity.ai/blog/sonar-ultra"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=82.5, normalized_score=0.88, source="https://www.perplexity.ai/blog/sonar-ultra"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=93.3, normalized_score=0.93, source="https://www.perplexity.ai/blog/sonar-ultra"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=8.1, normalized_score=0.81, source="https://www.perplexity.ai/blog/sonar-ultra"),
        ],
        pricing=[PricingTierRead(tier="requests", price_per_million=0.9, currency="USD")],
        context=ModelContextRead(
            max_tokens=160_000,
            modalities=["text"],
            notes="Hybrid search + generation model with autonomous browsing agents and citation graph outputs.",
        ),
        website="https://www.perplexity.ai/blog/sonar-ultra",
        summary="Perplexity's research assistant model combining retrieval, browsing agents, and verified citations.",
        sources=[
            "https://www.perplexity.ai/blog/sonar-ultra",
            "https://docs.perplexity.ai",
        ],
    ),
    ModelProfileRead(
        id="llama-4-120b",
        name="Llama 4 120B Instruct",
        provider="Meta",
        release_date=date(2025, 4, 18),
        context_window=128_000,
        license="apache-2.0",
        availability="self-host",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=86.7, normalized_score=0.935, source="https://ai.meta.com/blog/llama-4/"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=91.8, normalized_score=0.948, source="https://ai.meta.com/blog/llama-4/"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=84.6, normalized_score=0.896, source="https://ai.meta.com/blog/llama-4/"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=93.7, normalized_score=0.935, source="https://ai.meta.com/blog/llama-4/"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=7.8, normalized_score=0.78, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[],
        context=ModelContextRead(
            max_tokens=128_000,
            modalities=["text", "code"],
            notes="Open-weight flagship with streaming adapters for low-latency inference on commodity GPUs.",
        ),
        website="https://ai.meta.com/blog/llama-4/",
        summary="Meta's fourth generation open model with Responsible Use licensing and optional guard rails.",
        sources=[
            "https://ai.meta.com/blog/llama-4/",
            "https://github.com/meta-llama",
        ],
    ),
    ModelProfileRead(
        id="mistral-next-12x24b",
        name="Mistral Next 12×24B",
        provider="Mistral AI",
        release_date=date(2025, 4, 2),
        context_window=192_000,
        license="hybrid",
        availability="hybrid",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=85.1, normalized_score=0.922, source="https://mistral.ai/news/mistral-next-12x24b"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=90.1, normalized_score=0.939, source="https://mistral.ai/news/mistral-next-12x24b"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=82.0, normalized_score=0.885, source="https://mistral.ai/news/mistral-next-12x24b"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=92.8, normalized_score=0.928, source="https://mistral.ai/news/mistral-next-12x24b"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=7.6, normalized_score=0.76, source="https://lmsys.org/blog/2025-llm-leaderboard/"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=4.2, currency="USD"),
            PricingTierRead(tier="output", price_per_million=12.6, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=192_000,
            modalities=["text"],
            notes="MoE architecture tuned for retrieval-augmented workflows with configurable Retrieval Profiles.",
        ),
        website="https://mistral.ai/news/mistral-next-12x24b",
        summary="Mistral's retrieval-native mixture-of-experts release balancing latency and accuracy for enterprise RAG.",
        sources=[
            "https://mistral.ai/news/mistral-next-12x24b",
            "https://docs.mistral.ai",
        ],
    ),
    ModelProfileRead(
        id="phi-4",
        name="Phi-4",
        provider="Microsoft",
        release_date=date(2025, 5, 9),
        context_window=128_000,
        license="responsible-ai",
        availability="hybrid",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=82.4, normalized_score=0.9, source="https://blogs.microsoft.com/ai/introducing-phi-4"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=88.6, normalized_score=0.925, source="https://blogs.microsoft.com/ai/introducing-phi-4"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=78.9, normalized_score=0.86, source="https://blogs.microsoft.com/ai/introducing-phi-4"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=90.5, normalized_score=0.905, source="https://blogs.microsoft.com/ai/introducing-phi-4"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=7.4, normalized_score=0.74, source="https://blogs.microsoft.com/ai/introducing-phi-4"),
        ],
        pricing=[
            PricingTierRead(tier="input", price_per_million=1.8, currency="USD"),
            PricingTierRead(tier="output", price_per_million=5.4, currency="USD"),
        ],
        context=ModelContextRead(
            max_tokens=128_000,
            modalities=["text", "sensor"],
            notes="Edge-focused efficient transformer powering Copilot+ PCs, Azure Edge, and industrial inspection workloads.",
        ),
        website="https://blogs.microsoft.com/ai/introducing-phi-4",
        summary="Microsoft's compact-yet-powerful foundation model designed for on-device copilots and safety-critical edge deployments.",
        sources=[
            "https://blogs.microsoft.com/ai/introducing-phi-4",
            "https://learn.microsoft.com/azure/ai-studio/",
        ],
    ),
    ModelProfileRead(
        id="dbrx-2",
        name="DBRX 2 Instruct",
        provider="Databricks",
        release_date=date(2025, 3, 11),
        context_window=64_000,
        license="open-model",
        availability="self-host",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=83.4, normalized_score=0.908, source="https://www.databricks.com/blog/dbrx-2"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=89.9, normalized_score=0.936, source="https://www.databricks.com/blog/dbrx-2"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=81.1, normalized_score=0.872, source="https://www.databricks.com/blog/dbrx-2"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=91.7, normalized_score=0.917, source="https://www.databricks.com/blog/dbrx-2"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=7.7, normalized_score=0.77, source="https://www.databricks.com/blog/dbrx-2"),
        ],
        pricing=[PricingTierRead(tier="requests", price_per_million=0.65, currency="USD")],
        context=ModelContextRead(
            max_tokens=64_000,
            modalities=["text"],
            notes="MoE model optimised for Lakehouse retrieval and governed analytics workloads.",
        ),
        website="https://www.databricks.com/blog/dbrx-2",
        summary="Databricks' second generation open model tuned for SQL copilots, analytics assistants, and governed retrieval.",
        sources=[
            "https://www.databricks.com/blog/dbrx-2",
            "https://huggingface.co/databricks/dbrx-2-instruct",
        ],
    ),
    ModelProfileRead(
        id="mixtral-8x22b",
        name="Mixtral 8×22B",
        provider="Mistral AI",
        release_date=date(2024, 1, 15),
        context_window=65_536,
        license="apache-2.0",
        availability="self-host",
        benchmarks=[
            BenchmarkScoreRead(benchmark_id="mmlu", score=81.6, normalized_score=0.886, source="https://mistral.ai/news/mixtral-8x22b/"),
            BenchmarkScoreRead(benchmark_id="gsm8k", score=80.6, normalized_score=0.84, source="https://mistral.ai/news/mixtral-8x22b/"),
            BenchmarkScoreRead(benchmark_id="humaneval", score=73.5, normalized_score=0.79, source="https://mistral.ai/news/mixtral-8x22b/"),
            BenchmarkScoreRead(benchmark_id="hellaswag", score=87.2, normalized_score=0.872, source="https://mistral.ai/news/mixtral-8x22b/"),
            BenchmarkScoreRead(benchmark_id="mtbench", score=7.0, normalized_score=0.7, source="https://www.lightning.ai/pages/community/article/mt-bench-leaderboard/"),
        ],
        pricing=[],
        context=ModelContextRead(
            max_tokens=65_536,
            modalities=["text"],
            notes="Open-weight MoE classic that remains a popular baseline for self-hosted deployments.",
        ),
        website="https://mistral.ai/news/mixtral-8x22b/",
        summary="The original Mixtral release remains a strong open baseline for fine-tuning and governed inference.",
        sources=[
            "https://mistral.ai/news/mixtral-8x22b/",
            "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
        ],
    ),
]
