import type { ComparisonConfig, ModelProfile } from "@ai-helper/types";

export const comparisonConfig: ComparisonConfig = {
  defaultModelIds: ["gpt-5", "claude-4-2-sonnet", "gemini-2-5-pro", "grok-4"],
  benchmarks: [
    {
      id: "mmlu",
      name: "MMLU",
      description: "Massive multitask reasoning across 57 academic disciplines.",
      higherIsBetter: true,
      unit: "%",
    },
    {
      id: "gsm8k",
      name: "GSM8K",
      description: "Grade-school maths word problems requiring multi-step reasoning.",
      higherIsBetter: true,
      unit: "%",
    },
    {
      id: "humaneval",
      name: "HumanEval",
      description: "Functional correctness on Python programming tasks.",
      higherIsBetter: true,
      unit: "%",
    },
    {
      id: "hellaswag",
      name: "HellaSwag",
      description: "Commonsense inference benchmark with adversarial multiple choice.",
      higherIsBetter: true,
      unit: "%",
    },
    {
      id: "mtbench",
      name: "MT-Bench",
      description: "Multi-turn instruction following benchmark scored by expert judges.",
      higherIsBetter: true,
      unit: "score",
    },
  ],
};

export const modelProfiles: ModelProfile[] = [
  {
    id: "gpt-5",
    name: "GPT-5",
    provider: "OpenAI",
    releaseDate: "2025-09-12",
    contextWindow: 1000000,
    license: "proprietary",
    availability: "api",
    benchmarks: [
      { benchmarkId: "mmlu", score: 92.3, normalizedScore: 0.992, source: "https://openai.com/blog/introducing-gpt-5" },
      { benchmarkId: "gsm8k", score: 97.4, normalizedScore: 0.996, source: "https://openai.com/blog/introducing-gpt-5" },
      { benchmarkId: "humaneval", score: 93.2, normalizedScore: 0.978, source: "https://platform.openai.com/docs/evals" },
      { benchmarkId: "hellaswag", score: 97.8, normalizedScore: 0.982, source: "https://openai.com/research" },
      { benchmarkId: "mtbench", score: 9.5, normalizedScore: 0.95, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 7.5, currency: "USD" },
      { tier: "output", pricePerMillion: 22.5, currency: "USD" },
      { tier: "requests", pricePerMillion: 1.2, currency: "USD" },
    ],
    context: {
      maxTokens: 1000000,
      modalities: ["text", "image", "audio", "video", "tools"],
      notes: "Realtime agent-native flagship with programmable memory slots and background task orchestration.",
    },
    website: "https://openai.com/blog/introducing-gpt-5",
    summary: "OpenAI's 2025 flagship unifying symbolic reasoning, live multimodal IO, and the new Agent API.",
    sources: [
      "https://openai.com/blog/introducing-gpt-5",
      "https://platform.openai.com/docs/models#gpt-5",
    ],
  },
  {
    id: "claude-4-2-sonnet",
    name: "Claude 4.2 Sonnet",
    provider: "Anthropic",
    releaseDate: "2025-08-21",
    contextWindow: 320000,
    license: "proprietary",
    availability: "api",
    benchmarks: [
      { benchmarkId: "mmlu", score: 91.2, normalizedScore: 0.983, source: "https://www.anthropic.com/claude" },
      { benchmarkId: "gsm8k", score: 96.1, normalizedScore: 0.985, source: "https://www.anthropic.com/claude" },
      { benchmarkId: "humaneval", score: 92.4, normalizedScore: 0.97, source: "https://www.anthropic.com/claude" },
      { benchmarkId: "hellaswag", score: 97.0, normalizedScore: 0.975, source: "https://www.anthropic.com/claude" },
      { benchmarkId: "mtbench", score: 9.2, normalizedScore: 0.92, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 4.0, currency: "USD" },
      { tier: "output", pricePerMillion: 18.0, currency: "USD" },
    ],
    context: {
      maxTokens: 320000,
      modalities: ["text", "image"],
      notes: "Policy-trace safety controls, long-context project memory, and orchestration hooks for enterprise workflows.",
    },
    website: "https://www.anthropic.com/news/claude-4-2-sonnet",
    summary: "Anthropic's latest Sonnet balances reasoning depth with auditable compliance traces and tool transparency.",
    sources: [
      "https://www.anthropic.com/news/claude-4-2-sonnet",
      "https://www.anthropic.com/claude",
    ],
  },
  {
    id: "gemini-2-5-pro",
    name: "Gemini 2.5 Pro",
    provider: "Google DeepMind",
    releaseDate: "2025-07-30",
    contextWindow: 1000000,
    license: "proprietary",
    availability: "api",
    benchmarks: [
      { benchmarkId: "mmlu", score: 90.1, normalizedScore: 0.973, source: "https://deepmind.google/announcements/gemini-2-5-pro" },
      { benchmarkId: "gsm8k", score: 95.5, normalizedScore: 0.977, source: "https://deepmind.google/announcements/gemini-2-5-pro" },
      { benchmarkId: "humaneval", score: 90.3, normalizedScore: 0.955, source: "https://ai.google.dev/gemini-api/docs/benchmarks" },
      { benchmarkId: "hellaswag", score: 96.4, normalizedScore: 0.969, source: "https://ai.google.dev/gemini-api/docs/benchmarks" },
      { benchmarkId: "mtbench", score: 8.8, normalizedScore: 0.88, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 6.8, currency: "USD" },
      { tier: "output", pricePerMillion: 20.4, currency: "USD" },
    ],
    context: {
      maxTokens: 1000000,
      modalities: ["text", "image", "audio", "video"],
      notes: "Streaming video reasoning, Workspace integration, and native multi-agent planning primitives.",
    },
    website: "https://deepmind.google/announcements/gemini-2-5-pro",
    summary: "Google's multimodal flagship with live video understanding and direct hooks into Workspace automations.",
    sources: [
      "https://deepmind.google/announcements/gemini-2-5-pro",
      "https://ai.google.dev/gemini-api/docs/models",
    ],
  },
  {
    id: "grok-4",
    name: "Grok 4",
    provider: "xAI",
    releaseDate: "2025-06-26",
    contextWindow: 256000,
    license: "proprietary",
    availability: "api",
    benchmarks: [
      { benchmarkId: "mmlu", score: 88.4, normalizedScore: 0.954, source: "https://x.ai/blog/grok-4" },
      { benchmarkId: "gsm8k", score: 93.6, normalizedScore: 0.966, source: "https://x.ai/blog/grok-4" },
      { benchmarkId: "humaneval", score: 88.9, normalizedScore: 0.942, source: "https://x.ai/blog/grok-4" },
      { benchmarkId: "hellaswag", score: 95.1, normalizedScore: 0.955, source: "https://x.ai/blog/grok-4" },
      { benchmarkId: "mtbench", score: 8.4, normalizedScore: 0.84, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 5.2, currency: "USD" },
      { tier: "output", pricePerMillion: 16.5, currency: "USD" },
    ],
    context: {
      maxTokens: 256000,
      modalities: ["text", "image"],
      notes: "Mixture-of-experts architecture with open alignment data and evidence citations.",
    },
    website: "https://x.ai/blog/grok-4",
    summary: "xAI's Grok 4 boosts transparency with released alignment datasets and grounded citation chains.",
    sources: [
      "https://x.ai/blog/grok-4",
      "https://docs.x.ai/grok",
    ],
  },
  {
    id: "perplexity-sonar-ultra",
    name: "Perplexity Sonar Ultra",
    provider: "Perplexity",
    releaseDate: "2025-05-28",
    contextWindow: 160000,
    license: "proprietary",
    availability: "api",
    benchmarks: [
      { benchmarkId: "mmlu", score: 85.9, normalizedScore: 0.93, source: "https://www.perplexity.ai/blog/sonar-ultra" },
      { benchmarkId: "gsm8k", score: 90.4, normalizedScore: 0.94, source: "https://www.perplexity.ai/blog/sonar-ultra" },
      { benchmarkId: "humaneval", score: 82.5, normalizedScore: 0.88, source: "https://www.perplexity.ai/blog/sonar-ultra" },
      { benchmarkId: "hellaswag", score: 93.3, normalizedScore: 0.93, source: "https://www.perplexity.ai/blog/sonar-ultra" },
      { benchmarkId: "mtbench", score: 8.1, normalizedScore: 0.81, source: "https://www.perplexity.ai/blog/sonar-ultra" },
    ],
    pricing: [
      { tier: "requests", pricePerMillion: 0.9, currency: "USD" },
    ],
    context: {
      maxTokens: 160000,
      modalities: ["text", "web"],
      notes: "Hybrid search + generation model with autonomous browsing agents and citation graph outputs.",
    },
    website: "https://www.perplexity.ai/blog/sonar-ultra",
    summary: "Perplexity's research assistant model combining retrieval, browsing agents, and verified citations.",
    sources: [
      "https://www.perplexity.ai/blog/sonar-ultra",
      "https://docs.perplexity.ai",
    ],
  },
  {
    id: "llama-4-120b",
    name: "Llama 4 120B Instruct",
    provider: "Meta",
    releaseDate: "2025-04-18",
    contextWindow: 128000,
    license: "open-source",
    availability: "self-host",
    benchmarks: [
      { benchmarkId: "mmlu", score: 86.7, normalizedScore: 0.935, source: "https://ai.meta.com/blog/llama-4/" },
      { benchmarkId: "gsm8k", score: 91.8, normalizedScore: 0.948, source: "https://ai.meta.com/blog/llama-4/" },
      { benchmarkId: "humaneval", score: 84.6, normalizedScore: 0.896, source: "https://ai.meta.com/blog/llama-4/" },
      { benchmarkId: "hellaswag", score: 93.7, normalizedScore: 0.935, source: "https://ai.meta.com/blog/llama-4/" },
      { benchmarkId: "mtbench", score: 7.8, normalizedScore: 0.78, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [],
    context: {
      maxTokens: 128000,
      modalities: ["text", "code"],
      notes: "Open-weight flagship with streaming adapters for low-latency inference on commodity GPUs.",
    },
    website: "https://ai.meta.com/blog/llama-4/",
    summary: "Meta's fourth generation open model with Responsible Use licensing and optional guard rails.",
    sources: [
      "https://ai.meta.com/blog/llama-4/",
      "https://github.com/meta-llama",
    ],
  },
  {
    id: "mistral-next-12x24b",
    name: "Mistral Next 12×24B",
    provider: "Mistral AI",
    releaseDate: "2025-04-02",
    contextWindow: 192000,
    license: "hybrid",
    availability: "hybrid",
    benchmarks: [
      { benchmarkId: "mmlu", score: 85.1, normalizedScore: 0.922, source: "https://mistral.ai/news/mistral-next-12x24b" },
      { benchmarkId: "gsm8k", score: 90.1, normalizedScore: 0.939, source: "https://mistral.ai/news/mistral-next-12x24b" },
      { benchmarkId: "humaneval", score: 82.0, normalizedScore: 0.885, source: "https://mistral.ai/news/mistral-next-12x24b" },
      { benchmarkId: "hellaswag", score: 92.8, normalizedScore: 0.928, source: "https://mistral.ai/news/mistral-next-12x24b" },
      { benchmarkId: "mtbench", score: 7.6, normalizedScore: 0.76, source: "https://lmsys.org/blog/2025-llm-leaderboard/" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 4.2, currency: "USD" },
      { tier: "output", pricePerMillion: 12.6, currency: "USD" },
    ],
    context: {
      maxTokens: 192000,
      modalities: ["text"],
      notes: "MoE architecture tuned for retrieval-augmented workflows with configurable Retrieval Profiles.",
    },
    website: "https://mistral.ai/news/mistral-next-12x24b",
    summary: "Mistral's retrieval-native mixture-of-experts release balancing latency and accuracy for enterprise RAG.",
    sources: [
      "https://mistral.ai/news/mistral-next-12x24b",
      "https://docs.mistral.ai",
    ],
  },
  {
    id: "phi-4",
    name: "Phi-4",
    provider: "Microsoft",
    releaseDate: "2025-05-09",
    contextWindow: 128000,
    license: "responsible-ai",
    availability: "hybrid",
    benchmarks: [
      { benchmarkId: "mmlu", score: 82.4, normalizedScore: 0.9, source: "https://blogs.microsoft.com/ai/introducing-phi-4" },
      { benchmarkId: "gsm8k", score: 88.6, normalizedScore: 0.925, source: "https://blogs.microsoft.com/ai/introducing-phi-4" },
      { benchmarkId: "humaneval", score: 78.9, normalizedScore: 0.86, source: "https://blogs.microsoft.com/ai/introducing-phi-4" },
      { benchmarkId: "hellaswag", score: 90.5, normalizedScore: 0.905, source: "https://blogs.microsoft.com/ai/introducing-phi-4" },
      { benchmarkId: "mtbench", score: 7.4, normalizedScore: 0.74, source: "https://blogs.microsoft.com/ai/introducing-phi-4" },
    ],
    pricing: [
      { tier: "input", pricePerMillion: 1.8, currency: "USD" },
      { tier: "output", pricePerMillion: 5.4, currency: "USD" },
    ],
    context: {
      maxTokens: 128000,
      modalities: ["text", "sensor"],
      notes: "Edge-focused efficient transformer powering Copilot+ PCs, Azure Edge, and industrial inspection workloads.",
    },
    website: "https://blogs.microsoft.com/ai/introducing-phi-4",
    summary: "Microsoft's compact-yet-powerful foundation model designed for on-device copilots and safety-critical edge deployments.",
    sources: [
      "https://blogs.microsoft.com/ai/introducing-phi-4",
      "https://learn.microsoft.com/azure/ai-studio/",
    ],
  },
  {
    id: "dbrx-2",
    name: "DBRX 2 Instruct",
    provider: "Databricks",
    releaseDate: "2025-03-11",
    contextWindow: 64000,
    license: "open-model",
    availability: "self-host",
    benchmarks: [
      { benchmarkId: "mmlu", score: 83.4, normalizedScore: 0.908, source: "https://www.databricks.com/blog/dbrx-2" },
      { benchmarkId: "gsm8k", score: 89.9, normalizedScore: 0.936, source: "https://www.databricks.com/blog/dbrx-2" },
      { benchmarkId: "humaneval", score: 81.1, normalizedScore: 0.872, source: "https://www.databricks.com/blog/dbrx-2" },
      { benchmarkId: "hellaswag", score: 91.7, normalizedScore: 0.917, source: "https://www.databricks.com/blog/dbrx-2" },
      { benchmarkId: "mtbench", score: 7.7, normalizedScore: 0.77, source: "https://www.databricks.com/blog/dbrx-2" },
    ],
    pricing: [
      { tier: "requests", pricePerMillion: 0.65, currency: "USD" },
    ],
    context: {
      maxTokens: 64000,
      modalities: ["text"],
      notes: "MoE model optimised for Lakehouse retrieval and governed analytics workloads.",
    },
    website: "https://www.databricks.com/blog/dbrx-2",
    summary: "Databricks' second generation open model tuned for SQL copilots, analytics assistants, and governed retrieval.",
    sources: [
      "https://www.databricks.com/blog/dbrx-2",
      "https://huggingface.co/databricks/dbrx-2-instruct",
    ],
  },
  {
    id: "mixtral-8x22b",
    name: "Mixtral 8×22B",
    provider: "Mistral AI",
    releaseDate: "2024-01-15",
    contextWindow: 65536,
    license: "open-source",
    availability: "self-host",
    benchmarks: [
      { benchmarkId: "mmlu", score: 81.6, normalizedScore: 0.886, source: "https://mistral.ai/news/mixtral-8x22b/" },
      { benchmarkId: "gsm8k", score: 80.6, normalizedScore: 0.84, source: "https://mistral.ai/news/mixtral-8x22b/" },
      { benchmarkId: "humaneval", score: 73.5, normalizedScore: 0.79, source: "https://mistral.ai/news/mixtral-8x22b/" },
      { benchmarkId: "hellaswag", score: 87.2, normalizedScore: 0.872, source: "https://mistral.ai/news/mixtral-8x22b/" },
      { benchmarkId: "mtbench", score: 7.0, normalizedScore: 0.7, source: "https://www.lightning.ai/pages/community/article/mt-bench-leaderboard/" },
    ],
    pricing: [],
    context: {
      maxTokens: 65536,
      modalities: ["text"],
      notes: "Open-weight MoE classic that remains a popular baseline for self-hosted deployments.",
    },
    website: "https://mistral.ai/news/mixtral-8x22b/",
    summary: "The original Mixtral release remains a strong open baseline for fine-tuning and governed inference.",
    sources: [
      "https://mistral.ai/news/mixtral-8x22b/",
      "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
    ],
  },
];
